{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_train = pd.read_csv('Recipes.csv')\n",
    "y_train = pd.read_csv('Defects.csv')\n",
    "x_test = pd.read_csv('recipe_test_fixed.csv')\n",
    "defects_info = pd.read_csv('Defect_correlation_with_raw_materials.csv', sep=';')\n",
    "\n",
    "y_train.drop('PIECE_ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dosto\\Anaconda3\\envs\\cmc_mu_lbl\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# GET    false_codes\n",
    "CODES = defects_info['DEFECT_CODE'].tolist()\n",
    "\n",
    "false_codes = []\n",
    "for code in CODES:\n",
    "    row_in_df = defects_info.loc[defects_info['DEFECT_CODE'] == code ]\n",
    "    if row_in_df['FROM_RAW_MATERIAL'].item() == 'FALSE':\n",
    "        false_codes.append(code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12746\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE  HEATID_values\n",
    "HEATID_values = []\n",
    "HEATID_values = x_train['HEATID'].tolist()\n",
    "HEATID_values = set(HEATID_values)\n",
    "HEATID_values = sorted(HEATID_values)\n",
    "print(len(HEATID_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708\n"
     ]
    }
   ],
   "source": [
    "# DELETE HEAT ID with all FALSE codes\n",
    "all_false_codes = 0\n",
    "for value in HEATID_values:     \n",
    "    rowsDf = y_train.loc[y_train['HEAT_ID'] == value]\n",
    "    y = set(rowsDf['DEFECT_TYPE'].values.tolist())\n",
    "    if len(y) == 0:  # no defects\n",
    "        y={1}\n",
    "    y = list(y)\n",
    "\n",
    "    ok = False\n",
    "    for nn in range(0,len(y)):\n",
    "        if y[nn] not in false_codes:\n",
    "            ok=True\n",
    "    if ok == False:\n",
    "        x_train = x_train[x_train.HEATID != value]\n",
    "        y_train = y_train[y_train.HEAT_ID != value]\n",
    "        all_false_codes += 1\n",
    "\n",
    "print(all_false_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for CODE in false_codes:\n",
    "    y_train = y_train[y_train.DEFECT_TYPE != CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9038\n"
     ]
    }
   ],
   "source": [
    "# RECALCULATE  HEATID_values\n",
    "HEATID_values = []\n",
    "HEATID_values = x_train['HEATID'].tolist()\n",
    "HEATID_values = set(HEATID_values)\n",
    "HEATID_values = sorted(HEATID_values)\n",
    "print(len(HEATID_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# x_train  redefinition in some cell\n",
    "# ALL HEATID   ID sorted + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1_train=[]\n",
    "x2_train=[]\n",
    "labels=[]\n",
    "\n",
    "for value in HEATID_values:  \n",
    "    x1=[]\n",
    "    x2=[]\n",
    "\n",
    "    rowsDf = x_train.loc[x_train['HEATID'] == value ]\n",
    "    material = set( rowsDf['Material_Code'].values.tolist() )\n",
    "    material = list(material)#unique\n",
    "\n",
    "    matAndWeight = []\n",
    "    for single in material:\n",
    "        materialDf = rowsDf.loc[x_train['Material_Code'] == single ]\n",
    "        weight = sum(materialDf['WEIGHT'].values.tolist() )\n",
    "        x1.append(single)\n",
    "        x2.append(weight)\n",
    "\n",
    "    rowsDf = y_train.loc[y_train['HEAT_ID'] == value]\n",
    "    y = set( rowsDf['DEFECT_TYPE'].values.tolist() )\n",
    "    if( len(y)==0 ):#no defects\n",
    "        y={1}\n",
    "    y = list(y)\n",
    "\n",
    "    x1_train.append(x1)\n",
    "    x2_train.append(x2)\n",
    "    labels.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113, 114, 115]\n",
      "[50306.6, 3473.6, 2215.8]\n",
      "[59, 63, 71]\n"
     ]
    }
   ],
   "source": [
    "print(x1_train[1])\n",
    "print(x2_train[1])\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test heatid\n",
    "HEATID_values = []\n",
    "HEATID_values = x_test['HEATID'].tolist()\n",
    "HEATID_values = set(HEATID_values)\n",
    "HEATID_values = sorted(HEATID_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x1_test=[]\n",
    "x2_test=[]\n",
    "\n",
    "for value in HEATID_values:  \n",
    "    x1=[]\n",
    "    x2=[]\n",
    "\n",
    "    rowsDf = x_test.loc[x_test['HEATID'] == value ]\n",
    "    material = set( rowsDf['Material_Code'].values.tolist() )\n",
    "    material = list(material)#unique\n",
    "\n",
    "    matAndWeight = []\n",
    "    for single in material:\n",
    "        materialDf = rowsDf.loc[x_test['Material_Code'] == single ]\n",
    "        weight = sum(materialDf['WEIGHT'].values.tolist() )\n",
    "        x1.append(single)\n",
    "        x2.append(weight)\n",
    "\n",
    "    x1_test.append(x1)\n",
    "    x2_test.append(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_x2 = [x2_train, x2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "14\n",
      "2\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "length = []\n",
    "for a in x2_train:\n",
    "    length.append(len(a))     \n",
    "    \n",
    "print(min(length))\n",
    "print(max(length))\n",
    "\n",
    "length = []\n",
    "for a in x2_test:\n",
    "    length.append(len(a))     \n",
    "    \n",
    "print(min(length))\n",
    "print(max(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "TR_x1 = pad_sequences(x1_train, maxlen=14)\n",
    "TR_x2 = pad_sequences(x2_train, maxlen=14, dtype='float')\n",
    "TE_x1 = pad_sequences(x1_test, maxlen=14)\n",
    "TE_x2 = pad_sequences(x2_test, maxlen=14, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "for_scale = np.concatenate((TR_x2, TE_x2), axis=0)\n",
    "transformer = MinMaxScaler().fit_transform(for_scale)\n",
    "\n",
    "weightTR = transformer[0:len(x1_train)]\n",
    "weightTE = transformer[len(x1_train):len(transformer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "dlen = []\n",
    "for w in labels:\n",
    "    dlen.append(len(w))\n",
    "    \n",
    "print(max(dlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train['DEFECT_TYPE'].values.tolist() ))+1 # because of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "test = pd.Series(labels)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "label_onehoted = pd.DataFrame(mlb.fit_transform(test),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=test.index)\n",
    "#label_onehoted.to_csv('123.csv',index=False)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!  ALL MATERIAL CODES\n",
    "test = pd.Series(x1_train + x1_test)\n",
    "mlb = MultiLabelBinarizer()\n",
    "res1 = pd.DataFrame(mlb.fit_transform(test),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=test.index)\n",
    "#res.to_csv('tran.csv',index=False)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.Series(x1_train + x1_test)\n",
    "mlb = MultiLabelBinarizer()\n",
    "res2 = pd.DataFrame(mlb.fit_transform(test),\n",
    "                   columns=mlb.classes_,\n",
    "                   index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "allX2 = x2_train+x2_test\n",
    "flat_list = []\n",
    "for sublist in allX2:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "        \n",
    "flat_np = np.asarray(flat_list, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "z = res2.to_numpy(dtype=np.float64)\n",
    "\n",
    "for rows in range(len(z)):\n",
    "    for columns in range(len(z[0])):\n",
    "        if z[rows][columns] == 1:\n",
    "            z[rows][columns] = flat_np[0]\n",
    "            flat_np = np.delete(flat_np, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  FINAL VALUES\n",
    "X1_TRAIN = res1.to_numpy()[0:len(x1_train)]\n",
    "X1_TEST = res1.to_numpy()[len(x1_train):len(transformer)]\n",
    "X2_TRAIN = z[0:len(x1_train)]\n",
    "X2_TEST = z[len(x1_train):len(transformer)]\n",
    "\n",
    "LABELS = label_onehoted.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n",
    "# from sklearn.metrics import auc, accuracy_score, confusion_matrix#, mean_squared_error\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n",
    "\n",
    "\n",
    "def report_best_scores(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "f1_scorer = make_scorer(f1_score, average=\"micro\",greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 140 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 76.0min\n",
      "[Parallel(n_jobs=4)]: Done 420 out of 420 | elapsed: 170.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__booster': 'gbtree', 'estimator__colsample_bytree': 0.8095632951174561, 'estimator__gamma': 0.2520958161210098, 'estimator__learning_rate': 0.3017490596517869, 'estimator__max_depth': 13, 'estimator__min_child_weight': 7, 'estimator__n_estimators': 141, 'estimator__objective': 'binary:logistic', 'estimator__subsample': 0.6052065454109774}\n",
      "0.08336043738372087\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "xgb_model = OneVsRestClassifier(xgb.XGBClassifier())\n",
    "\n",
    "params = {\n",
    "    \"estimator__objective\": [\"binary:logistic\"],\n",
    "    \"estimator__booster\":['gbtree'],\n",
    "    \"estimator__min_child_weight\":[ 1, 3, 5, 7 ],\n",
    "    \"estimator__colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"estimator__gamma\": uniform(0, 0.5),\n",
    "    \"estimator__learning_rate\": uniform(0.03, 0.3),  \n",
    "    \"estimator__max_depth\": randint(2, 15),\n",
    "    \"estimator__n_estimators\": randint(100, 150), \n",
    "    \"estimator__subsample\": uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, \n",
    "                            n_iter=140, verbose=1,\n",
    "                            return_train_score=True,\n",
    "                            scoring=f1_scorer, n_jobs=4 )\n",
    "search.fit(X2_TRAIN, LABELS)\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "\n",
    "print (search.best_params_)\n",
    "print (search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dosto\\Anaconda3\\envs\\cmc_mu_lbl\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 5 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "C:\\Users\\dosto\\Anaconda3\\envs\\cmc_mu_lbl\\lib\\site-packages\\sklearn\\multiclass.py:76: UserWarning: Label not 95 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25176304654442877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X, X_val, Y, Y_val = train_test_split(X2_TRAIN, LABELS, test_size=0.2,random_state=52 , shuffle = True) #random_state=52\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classif = OneVsRestClassifier(DecisionTreeClassifier())\n",
    "classif.fit(X, Y)\n",
    "predicted_values = classif.predict(X_val) \n",
    "print(f1_score(Y_val, predicted_values , average=\"micro\"))  # LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#               CROSS VALIDATION SCIKIT-LEARN\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier,SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "X = X2_TRAIN\n",
    "Y = LABELS\n",
    "\n",
    "kfold = KFold(n_splits=10,shuffle=True,random_state=seed)\n",
    "\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "    #   ONLY FOR VOTING\n",
    "    #m1 = OneVsRestClassifier( KNeighborsClassifier(n_neighbors=3)  )\n",
    "    #m2 = OneVsRestClassifier( ExtraTreesClassifier(n_estimators = 50)  )\n",
    "    m3 = OneVsRestClassifier( RandomForestClassifier(n_estimators=50, random_state= 101) ) \n",
    "    #m4 = OneVsRestClassifier( DecisionTreeClassifier() ) \n",
    "    #m5 = OneVsRestClassifier( SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=250)  ) \n",
    "    \n",
    "    #from sklearn.ensemble import VotingClassifier\n",
    "    #clf = [m1, m2, m3, m4]\n",
    "    #eclf = VotingClassifier(estimators=[('m1', m1), ('m2', m2), ('m3', m3), ('m4', m4)], voting='soft')\n",
    "\n",
    "    classif = OneVsRestClassifier( RandomForestClassifier(n_estimators=50, random_state= 101) ) \n",
    "    classif.fit(X[train], Y[train])\n",
    "\n",
    "    y_pred = classif.predict(X[test])   \n",
    "    print(y_pred[0])\n",
    "    #y_pred1 = classif.predict(X[test])\n",
    "    #y_pred = np.zeros(y_pred1.shape)\n",
    "    #y_pred[y_pred1>0.3] = 1 \n",
    "    \n",
    "    scores = f1_score(Y[test], y_pred , average=\"micro\")\n",
    "    print(\"%s: %.3f%%\" % ('f1 micro-average: ', scores))\n",
    "    cvscores.append(scores)\n",
    "\n",
    "print(\"%.3f%% (+/- %.3f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {\n",
    "       'estimator__kernel': ['linear','poly','rbf','sigmoid','precomputed'],  \n",
    "       'estimator__gamma': ['auto','scale'],  \n",
    "       'estimator__shrinking': [True,False],  \n",
    "    #   'estimator__cache_size': [1,2,8,32,64],  \n",
    "       'estimator__decision_function_shape': ['ovr'],   \n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, average=\"micro\",greater_is_better=True)\n",
    "\n",
    "clf = RandomizedSearchCV( OneVsRestClassifier(SVC(verbose=1)),\n",
    "                          parameters, scoring=f1_scorer,n_jobs=4,verbose=1,n_iter = 1)\n",
    "\n",
    "clf.fit(X2_TRAIN, LABELS)\n",
    "print (clf.best_params_)\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#m1 = OneVsRestClassifier( KNeighborsClassifier(n_neighbors=3)  )\n",
    "#m2 = OneVsRestClassifier( ExtraTreesClassifier(n_estimators = 50)  )\n",
    "#m3 = OneVsRestClassifier( RandomForestClassifier(n_estimators=50) ) \n",
    "#m4 = OneVsRestClassifier( Perceptron() )\n",
    "#m5 = OneVsRestClassifier( DecisionTreeClassifier() ) \n",
    "#m6 = OneVsRestClassifier( SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=250) )\n",
    "m7 = OneVsRestClassifier( MLPClassifier() )  #??? \n",
    "SVM\n",
    "Linear\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3729081565423639\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_val, Y, Y_val = train_test_split(X2_TRAIN, LABELS, test_size=0.2,random_state=52 , shuffle = True)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import xgboost as xgb\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,BatchNormalization\n",
    "\n",
    "input1 = Input(shape=(29,) )\n",
    "dense1 = Dense(400, activation=\"elu\")(input1)\n",
    "bn1 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(400, activation=\"elu\")(bn1)\n",
    "bn2 = BatchNormalization()(dense2)\n",
    "dense3 = Dense(400, activation=\"elu\")(bn2)\n",
    "bn3 = BatchNormalization()(dense3)\n",
    "output = Dense(123, activation=\"sigmoid\")(bn3)\n",
    "model_C =  Model(inputs=input1, outputs=output)\n",
    "\n",
    "model_C.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "model_C.fit(X, Y, batch_size=64, epochs=24,verbose=0 )\n",
    "\n",
    "y_pred1 = model_C.predict(X_val)  \n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.25] = 1 \n",
    "scores = f1_score(Y_val, y_pred , average=\"micro\")\n",
    "print(scores)\n",
    "\n",
    "#mod1=xgb.XGBClassifier(booster= 'gbtree', colsample_bytree= 0.809,  gamma= 0.252,\n",
    "#                      learning_rate=0.301,  max_depth= 15, min_child_weight= 7, n_estimators = 141,  \n",
    "#                      objective = 'binary:logistic', subsample= 0.605, n_jobs=4)\n",
    "\n",
    "\n",
    "\n",
    "#clf1 = OneVsRestClassifier( KNeighborsClassifier(weights= 'uniform', p=1, n_neighbors=3, leaf_size=10, algorithm  = 'ball_tree') )\n",
    "#clf2 = OneVsRestClassifier(  mod1  )\n",
    "#lf3 = OneVsRestClassifier(  mod2  )\n",
    "\n",
    "\n",
    "#clf_test1.fit(X, Y)\n",
    "#y_pred1 = clf_test1.predict(X_val)  \n",
    "#y_pred = np.zeros(y_pred1.shape)\n",
    "#y_pred[y_pred1>0.3] = 1 \n",
    "#scores = f1_score(Y_val, y_pred , average=\"micro\")\n",
    "#print(scores)\n",
    "\n",
    "#y_pred1 = clf_test2.predict(X_val)  \n",
    "#y_pred = np.zeros(y_pred1.shape)\n",
    "#y_pred[y_pred1>0.3] = 1 \n",
    "#scores = f1_score(Y_val, y_pred , average=\"micro\")\n",
    "#print(scores)\n",
    "\n",
    "\n",
    "#0.35413802584547704 35\n",
    "#0.36283429831816927 30\n",
    "\n",
    "#0.36424024356490453 25\n",
    "#0.36843544857768046 24\n",
    "#0.3660701704945752 24\n",
    "\n",
    "\n",
    "#0.3652656821910892\n",
    "#0.3693716611549224\n",
    "#0.36969539182504557\n",
    "#0.37216455209534793"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3742323599448552\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = model_C.predict(X_val)  \n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.23] = 1 \n",
    "scores = f1_score(Y_val, y_pred, average=\"micro\")\n",
    "print(scores)\n",
    "\n",
    "#0.22   0.37333333333333335\n",
    "\n",
    "#elu\n",
    "#0.23 0.3742323599448552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense,BatchNormalization\n",
    "from sklearn.linear_model import SGDClassifier,Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, X_val, Y, Y_val = train_test_split(X2_TRAIN, LABELS, test_size=0.2, random_state=52, shuffle = True)\n",
    "\n",
    "#Xtr, X_meta, Ytr, Y_meta = train_test_split(  X, Y, test_size=0.2,random_state=52 , shuffle = True)\n",
    "iteration = 0\n",
    "#(7230, 123)\n",
    "pred_values1 = []\n",
    "pred_values2 = []\n",
    "pred_values3 = []\n",
    "pred_values4 = []\n",
    "pred_values5 = []\n",
    "pred_values6 = []\n",
    "pred_values7 = []\n",
    "pred_values8 = []\n",
    "GT_values = []\n",
    "\n",
    "kfold = KFold(n_splits=10,shuffle=True,random_state=101)\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    iteration += 1\n",
    "    print(iteration)\n",
    "        \n",
    "    clf1 = OneVsRestClassifier( KNeighborsClassifier(weights= 'uniform', p=1, n_neighbors=3, leaf_size=10, algorithm  = 'ball_tree') )  \n",
    "    clf2 = OneVsRestClassifier( ExtraTreesClassifier(n_estimators=75) ) \n",
    "    clf3 = OneVsRestClassifier( RandomForestClassifier(n_estimators= 5, max_features= None, criterion= 'entropy', class_weight=None)  )\n",
    "    clf7 = OneVsRestClassifier( xgb.XGBClassifier(booster= 'gbtree', colsample_bytree= 0.809,  gamma= 0.252,learning_rate=0.301, max_depth= 15,\n",
    "                                                  min_child_weight= 7, n_estimators = 141,  objective = 'binary:logistic', subsample= 0.605, n_jobs=4) )\n",
    "    \n",
    "    input1 = Input(shape=(29,) )\n",
    "    dense1 = Dense(400, activation=\"relu\")(input1)\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    dense2 = Dense(400, activation=\"relu\")(bn1)\n",
    "    bn2 = BatchNormalization()(dense2)# \n",
    "    dense3 = Dense(400, activation=\"relu\")(bn2)\n",
    "    bn3 = BatchNormalization()(dense3)\n",
    "\n",
    "    output = Dense(123, activation=\"sigmoid\")(bn3)\n",
    "    clf8 = Model(inputs=input1, outputs=output)\n",
    "    clf8.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "\n",
    "    \n",
    "    clf1.fit(X[train], Y[train])\n",
    "    clf2.fit(X[train], Y[train])\n",
    "    clf3.fit(X[train], Y[train])\n",
    "    clf7.fit(X[train], Y[train])\n",
    "    clf8.fit(X[train], Y[train], batch_size=64, epochs=24,verbose=0 )\n",
    "    \n",
    "    pred_values1.append(clf1.predict(X[test]) )  \n",
    "    pred_values2.append(clf2.predict(X[test]) )  \n",
    "    pred_values3.append(clf3.predict(X[test]) )  \n",
    "    pred_values7.append(clf7.predict(X[test]) ) \n",
    "    y_pred1 = clf8.predict(X[test])\n",
    "    y_pred = np.zeros(y_pred1.shape)\n",
    "    y_pred[y_pred1>0.22] = 1.0 \n",
    "    pred_values8.append(y_pred)  \n",
    "    \n",
    "    GT_values.append(Y[test])\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "a1 = np.concatenate(pred_values1, axis=0)\n",
    "a2 = np.concatenate(pred_values2, axis=0)\n",
    "a3 = np.concatenate(pred_values3, axis=0)\n",
    "a7 = np.concatenate(pred_values7, axis=0)\n",
    "a8 = np.concatenate(pred_values8, axis=0)\n",
    "\n",
    "Y_meta=np.concatenate(GT_values, axis=0)\n",
    "\n",
    "# VAL\n",
    "a1_meta = clf1.predict(X_val)\n",
    "a2_meta = clf2.predict(X_val)\n",
    "a3_meta = clf3.predict(X_val)\n",
    "a7_meta = clf7.predict(X_val)\n",
    "y_pred1 = clf8.predict(X_val)\n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.22] = 1.0 \n",
    "a8_meta = y_pred\n",
    "\n",
    "# TEST\n",
    "t1 = clf1.predict(X2_TEST)\n",
    "t2 = clf2.predict(X2_TEST)\n",
    "t3 = clf3.predict(X2_TEST)\n",
    "t7 = clf7.predict(X2_TEST)\n",
    "y_pred1 = clf8.predict(X2_TEST)\n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.22] = 1.0 \n",
    "t8 = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stacked_input = np.stack((a1,a2,a3,a7,a8), axis=1)\n",
    "stacked_val = np.stack((a1_meta, a2_meta,a3_meta,a7_meta,a8_meta), axis=1)\n",
    "stacked_test = np.stack((t1,t2,t3,t7,t8), axis=1)\n",
    "# train shape (7230, 3, 123)\n",
    "# test shape (1808, 3, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 micro-average: : 0.373%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, SpatialDropout1D, Concatenate,BatchNormalization,Flatten,Dropout\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "models_num = 5\n",
    "nodes = 123 * 8\n",
    "\n",
    "finalscore = []\n",
    "for itr in range(0,1):\n",
    "    input1 = Input(shape=(models_num,123)  )\n",
    "    dense1 = Dense(123, activation=\"relu\")(input1)\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    dr1 = Dropout(0.1)(bn1)\n",
    "\n",
    "    #dense2 = Dense(123, activation=\"relu\")(bn1)\n",
    "    #bn2 = BatchNormalization()(dense2)\n",
    "    #dr2 = Dropout(0.1)(bn2)\n",
    "    #dense3 = Dense(123, activation=\"relu\")(bn2)\n",
    "    #bn3 = BatchNormalization()(dense3)\n",
    "\n",
    "    flt= Flatten()(dr1)\n",
    "    output = Dense(123, activation=\"sigmoid\")(flt)\n",
    "    meta_model =  Model(inputs=input1, outputs=output)\n",
    "\n",
    "    meta_model.compile(loss='binary_crossentropy', \n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    meta_model.fit(x=stacked_input, y=Y_meta, batch_size=32, epochs=4,verbose=0 ) #,  callbacks=[early_stop,reduce_lr]\n",
    "\n",
    "    y_pred1 = meta_model.predict(stacked_val, verbose=0)\n",
    "    y_pred = np.zeros(y_pred1.shape)\n",
    "    y_pred[y_pred1>0.3] = 1 \n",
    "    scores = f1_score(Y_val, y_pred , average=\"micro\")\n",
    "    finalscore.append(scores)\n",
    "    print(\"%s: %.3f%%\" % ('f1 micro-average: ', scores))\n",
    "    \n",
    "# print(\"%.3f%% (+/- %.3f%%)\" % (np.mean(finalscore), np.std(finalscore)))\n",
    "# 0.3733333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38002960570582694\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = meta_model.predict(stacked_val, verbose=0)\n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.26] = 1 \n",
    "scores = f1_score(Y_val, y_pred, average=\"micro\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-33-1f6bb3b8e50a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-33-1f6bb3b8e50a>\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    64 batch 4 epoch\u001B[0m\n\u001B[1;37m           ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "64 batch 4 epoch\n",
    "_________________\n",
    "0.34\n",
    "0.334\n",
    "0.33\n",
    "\n",
    "\n",
    "\n",
    "x1 984 ---\n",
    "0.33482267523038256\n",
    "0.3122551762730834\n",
    "\n",
    "x1 123\n",
    "0.3441164006215567\n",
    "0.3268612934767426\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1 123\n",
    "x2 123\n",
    "0.334876989869754\n",
    "0.309953434225844\n",
    "#0.32238135355237285  epoch 3   600\n",
    "#0.3300403506330875   epoch 3   700\n",
    "\n",
    "# WORK!\n",
    "# need more than 0.3145503060743996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris\n",
    "seed = 2017\n",
    "np.random.seed(seed)\n",
    "from mlens.ensemble import SuperLearner\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "X, X_val, Y, Y_val = train_test_split(  X2_TRAIN, LABELS, test_size=0.2,random_state=52 , shuffle = True) #random_state=52 \n",
    "from mlens.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score #, make_scorer\n",
    "f1_scorer = make_scorer(f1_score, average=\"micro\",greater_is_better=True)\n",
    "ensemble = SuperLearner(scorer=f1_scorer, random_state=seed, verbose=2)\n",
    "ensemble.add([OneVsRestClassifier(RandomForestClassifier(random_state=seed)), OneVsRestClassifier(SVC())])\n",
    "ensemble.add_meta(OneVsRestClassifier(LogisticRegression()))\n",
    "ensemble.fit(X, Y)\n",
    "preds = ensemble.predict(X_val)\n",
    "print(\"Fit data:\\n%r\" % ensemble.data)\n",
    "print(\"Prediction score: %.3f\" % accuracy_score(preds, Y_val))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  SAVE PREDICTIONS TO CSV\n",
    "\n",
    "y_pred1 = meta_model.predict(stacked_test)\n",
    "#y_pred1 = model_C.predict(X2_TEST)\n",
    "\n",
    "y_pred = np.zeros(y_pred1.shape)\n",
    "y_pred[y_pred1>0.26] = 1 \n",
    "\n",
    "defectCodes = []\n",
    "for each in label_onehoted:\n",
    "    defectCodes.append(each)\n",
    "\n",
    "predictions = []\n",
    "column_num = len(y_pred[0])\n",
    "\n",
    "for row in range(0, len(y_pred)):\n",
    "    temp_list = []\n",
    "    for column in range(0, column_num):\n",
    "        if y_pred[row][column] == 1:\n",
    "            temp_list.append(defectCodes[column])\n",
    "            \n",
    "    if 1 in temp_list:\n",
    "        temp_list = []\n",
    "        \n",
    "    final_str = ''\n",
    "    if len(temp_list) != 0:\n",
    "        for e in temp_list:\n",
    "            final_str += ','+str(e)\n",
    "    else:\n",
    "        final_str = ','\n",
    "\n",
    "    predictions.append(final_str)    \n",
    "\n",
    "    \n",
    "from collections import OrderedDict\n",
    "HEATID = x_test['HEATID'].values.tolist()\n",
    "HEATID = list(OrderedDict.fromkeys(HEATID))\n",
    "\n",
    "final_pred = []\n",
    "for z in range(0, len(HEATID)):\n",
    "    final_pred.append( str(HEATID[z]) + predictions[z] )\n",
    "\n",
    "with open('predictions.csv','w') as file:\n",
    "    for line in final_pred:\n",
    "        file.write(line)\n",
    "        file.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}